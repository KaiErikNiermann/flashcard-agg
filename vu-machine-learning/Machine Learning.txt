#separator:tab
#html:true
reinforcement learning	"the study of true learning agents, we define : agent, environment, reward system ; agent must learn to<span style=""font-weight:600""> explore environment and maximize rewards </span>here we are using a <span style=""font-weight:600"">feedback system</span>"
offline learning	"main idea is the <span style=""font-weight:600"">separation of the model learning</span> process <span style=""font-weight:600"">using a fixed dataset</span> and using the learned predictions to take certain actions"
"<span style=""font-weight:600"">bias and variance</span>"	"bias is the difference between the <span style=""font-weight:600"">optimal error and the true error </span>of your model ; variance is the difference between the <span style=""font-weight:600"">measured error and the true error</span>"
"<span style=""font-weight:600"">boosting</span>"	"boosting is a <span style=""font-weight:600"">bias reduction technique</span> of combining models with a generally high bias into a group of models ( that functions as 1 ) which itself has a lower bias but at the cost of a higher variance"
<b>training labels</b>	"These are the <span style=""font-weight:600"">targets </span>associated with your <span style=""font-weight:600"">instances/examples </span>so what you are training towards for a given input into your model"
"<span style=""font-weight:600"">backpropagation</span>"	"method of <span style=""font-weight:600"">training neural networks </span>where we use the <span style=""font-weight:600"">chain rule </span>to compute the <span style=""font-weight:600"">gradient of the loss function </span>with respect to the <span style=""font-weight:600"">weights of the network&nbsp;</span>"
"<span style=""font-weight:600"">symbolic computation</span>"	"the manipulation of algebraic expressions (<span style=""font-weight:600"">such as the backpropagation equation</span>) generally to compute their result e.g. applying derivative rules"
<b>cost imbalance</b>	A measure of how much worse a mislabeled positive is rather than a mislabeled negative ( e.g. sending a sick person home vs applying an invasive test to a healthy person  )
"<span style=""font-weight:600"">multiple testing</span>"	"This is where you <span style=""font-weight:600"">test so many things </span>that the likelihood of a <span style=""font-weight:600"">noticeable effect </span>popping up <span style=""font-weight:600"">by chance increases</span>."
classifier	"A machine learning which takes as an <span style=""font-weight:600"">input some type of instance </span>(e.g. emails) and <span style=""font-weight:600"">assigns this input to a particular class </span>(e.g. spam or ham)"
test set	"The set of instances which you use to <span style=""font-weight:600"">test your models prediction capabilities </span>on <span style=""font-weight:600"">once&nbsp;</span>"
precision	"The proportion of <span style=""font-weight:600"">classified positive </span>examples that where <span style=""font-style:italic;font-weight:600"">actually </span><span style=""font-weight:600"">positive</span>."
false positive	"The proportion of <span style=""font-weight:600"">classified positive </span>examples that where <span style=""font-style:italic;font-weight:600"">actually </span><span style=""font-weight:600"">negative.</span>"
confusion matrix	"A grid comparing the <span style=""font-weight:600"">actual labels (y-axis) </span>and <span style=""font-weight:600"">predicted labels (x-axis) </span>; e.g. if you predicted 6 instances as positive and they where all actually positive you would have 6 at the cell indexes (pos, pos)"
"<span style=""font-weight:600"">ROC space</span>"	"A graph where each point represents the <span style=""font-weight:600"">true positive rate (y-axis) </span>and <span style=""font-weight:600"">false positive rate (x-axis) </span>of a classifier, this space is the ROC-space"
"<span style=""font-weight:600"">Ranking classifier</span>"	"A ranking classifier express a <span style=""font-weight:600"">particular class </span>and <span style=""font-weight:600"">how positive </span>(confident) this prediction is (e.g. linear classifier confidence could be indicated by the distance to the decision boundary)"
"<span style=""font-weight:600"">ROC curve</span>"	The ROC-curve is a curve generated by drawing lines between all points in the ROC-space, and then considering only the area below these lines which forms the area under the ROC-curve
ROC area under curve	We can create any classifier in the points contained in the area under the ROC-curve, this area is defined by a classifier as it becomes very strict in labelled something positive to slowly becoming more generous ; this behavior is defined by the ranking classifier
withholding test set	this is the basic idea of not using your test set until the very end to avoid the issue of multiple testing
train/validation/test split	the idea is that you should split your data into training and validation for hyperparameter selection and testing to test your hypothesis
normalizing data	"the idea behind normalizing is that we <span style=""font-weight:600"">reshape the data</span> to either be <span style=""font-weight:600"">constrained within [0, 1]</span> or be <span style=""font-weight:600"">normally distributed</span> - we do this because we <span style=""font-weight:600"">don’t care about natural units</span>, we<span style=""font-weight:600""> care about how close</span> our points are <span style=""font-weight:600"">to the target compared to the other points</span>"
bootstrap sampling	"Bootstrapping or bootstrap sampling is the process of <span style=""font-weight:600"">sampling with replacement</span> a <span style=""font-weight:600"">dataset </span>of the <span style=""font-weight:600"">same size as the whole data set</span> - this sample can help you <span style=""font-weight:600"">get an idea of the bias/variance tradeoff</span> and <span style=""font-weight:600"">help </span>you <span style=""font-weight:600"">build an ensemble</span>"
"<span style=""font-weight:600"">boosted ensemble</span>"	"Boosting is the idea of creating an ensemble (group) of models that individually have a high bias but as a group have a low bias ; it is a <span style=""font-weight:600"">bias mitigation </span>technique"
Imputation	"processes of <span style=""font-weight:600"">filling in the missing values </span>in the test or training data, you should use different metrics of the data as filler depending on the type of data : categorical data → use <span style=""font-weight:600"">mode </span>; numeric data → use <span style=""font-weight:600"">mean/median</span>"
Standardization	"Reshapes the data so that its <span style=""font-weight:600"">mean </span>and <span style=""font-weight:600"">variance </span>are those of a standard normal distribution (0 and 1 respectively)"
Normalization	Reshapes all values to lie within a range of [0,1 ]​
Whitening	Standardization across all features
Principle Component Analysis	"A <span style=""font-weight:600"">method of dimensionality reduction </span>; We take information from all features and map this to a smaller set of derived features ; picking the same number of components as original <span style=""font-weight:600"">normalizes the data</span>"
autoencoder	"A type of NN used to learn <span style=""font-weight:600"">efficient encodings </span>of <span style=""font-weight:600"">unlabeled data </span>; encoding function <span style=""font-weight:600"">input data → encoded representation ; </span>decoding function <span style=""font-weight:600"">encoded representation → input data</span> ; typically used for <span style=""font-weight:600"">dimensionality reduction</span>"
variational autoencoder	"variation of an autoencoder with a <span style=""font-weight:600"">probabilistic latent space </span>; encoded input → distribution in latent space of input → decoded output sampled from distribution<span style=""font-weight:600""></span>"
sampling step	when reconstructing the input ( decoding phase ) a VAE samples from the distribution of the encoded input to generate the output
encoder and decoder	in the encoding phase we compress some input ( e.g. image ) to a compressed “encoded” form ; for decoding the goal is to reconstruct the image from the compressed representation as close as possible
latent space	The input space of a generator network, typically sampled from a standard multivariate normal distribution.
"<span style=""font-weight:600"">generator model</span>"	A model which summarizes the distribution of input variables to potentially generate new examples ( e.g. by sampling from this distribution )
discriminator model	A model which discriminates examples of input variables across classes to choose a specific class label, also knows as classification
gradient descent	The optimization technique of descending (finding the minimum) the gradient (slope) of the loss function ( aka the loss surface/landscape ) ; commonly done using calculus to find the direction of the minimum to move towards
loss landscape	a plotted graph of the loss values of all models hyperparameter configurations ; used to find the optimal model ( with the minimal loss )
non-differentiable step/loss in RL	one of the key problems with reinforcement learning is a non-differentiable loss so a non-differentiable step between the output and the reward
reason for squaring error	"two main reasons : <span style=""font-weight:600"">avoid negative and positive residual cancelling each other out </span>, <span style=""font-weight:600"">ensure larger residuals effect loss more heavily than smaller ones , ensure 0 or positive error terms&nbsp;</span>"
decision boundary	"A line or hyperplane <span style=""font-weight:600"">in the feature space </span>which the purpose of <span style=""font-weight:600"">separating the instances </span>generally into <span style=""font-weight:600"">distinct classes </span>(e.g. above line = ham email, below line = spam email)"
assuming normally distributed errors	we can usually assume that some value m which can represent the error of a model (e.g. MSE loss of a regression model) is normally distributed ; so for example if we plot the distribution of errors for all instances most of these errors will hover around their average AKA be normally distributed
maximum likelihood solution	"model criteria for which we <span style=""font-weight:600"">guess the model </span>for which the probability of seeing the data that we saw was the highest ; deriving the maximum likelihood solution in practical terms often means <span style=""font-weight:600"">minimizing the errors</span> of the model"
features	"Something <span style=""font-weight:600"">measured from an instance</span> (e.g. pixel rgb values from an image instance)"
instances	"The <span style=""font-weight:600"">examples from the training data</span> that the machine model uses to learn these measurable features"
gradient	"The gradient points in the direction of steepest ascent ; in the context of the loss surface this means the direction the loss <span style=""font-weight:600"">increases the fastest </span>hence why to minimize the loss we do <span style=""font-weight:600"">gradient </span><span style=""font-style:italic;font-weight:600"">descent</span>"
random search	"An <span style=""font-weight:600"">optimization algorithm</span> (generally over the loss surface) to <span style=""font-weight:600"">find the minimum</span> by <span style=""font-weight:600"">picking a random point → going to it and moving back if the loss goes up</span> otherwise repeating ; <span style=""font-weight:600"">step size is randomly picked </span>within a surrounding hypersphere or sampled from a distribution ; works with <span style=""font-weight:600"">non-differentiable models</span>"
differentiable model	A model is differentiable if its hyperparameters (e.g. weights) can be updated using backpropagation
convex loss surface	"also sometimes called <span style=""font-weight:600"">convex problem </span>refers to the case where the loss surface is <span style=""font-weight:600"">convex </span>AKA <span style=""font-weight:600"">bowl shaped</span> ; key property is a <span style=""font-weight:600"">single global minima&nbsp;</span>"
evolutionary models	"also sometimes called algorithms are a type of <span style=""font-weight:600"">population method </span>which are methods that use a <span style=""font-weight:600"">population of searching agents </span>that <span style=""font-weight:600"">communicate </span>with each other in the search of a minimum ; they are <span style=""font-weight:600"">powerful and easy to parallelize </span>but <span style=""font-style:italic;font-weight:600"">can </span><span style=""font-weight:600"">be slow and hard to tune</span>"
local and global minima	a minima is a point of a function where the tangent line / hyperplane has a slope of 0, global minima is the point which exhibits this property and is also the minimal value of the function for all other minima, local minima are points with this property ( f'(x)=0 ) that are not as small as the global minima
kNN classifier	"K-nearest neighbor is a<span style=""font-weight:600""> </span><span style=""font-style:italic;font-weight:600"">lazy</span><span style=""font-weight:600""> classifier</span>, it doesn’t do any actual learning it just <span style=""font-weight:600"">remembers the data</span>. For a new point we look at the k closest points on the feature space and assign to this new point the class that it most frequent in this neighborhood set."
kNN key mistake	"<span style=""font-weight:600"">Multiple testing</span>, if we do multiple testing and let this influence our choice of k then its more likely for just some specific property of a dataset to give us a low error score rather than any meaninful property of the data"
hyperparameter	Just a parameter of the model, such as the weights for neural networks or the choice of k in kNN classifiers
grid search	type of automated hyperparameter search where exhaustively check a grid of hyperparameters
overfitting	learning the training data instead of creating a model which can generalize well to different types of unseen data
multiple testing problem	This is where you test so many things that the likelihood of a noticeable effect popping up by chance increases.
random (hyperparameter) search	"type of automated hyperparameter search where we randomly sample a combination from the parameter space ; this is <span style=""font-weight:600"">better than grid (exhaustive) search</span>"
accuracy performance metric	"Accuracy measures the <span style=""font-weight:600"">proportion of correctly classified examples</span>"
class imbalance	"Class imbalance refers to the case where the<span style=""font-weight:600""> class of something we are looking at has a very unequal distribution</span> making it hard to determine if a classifier actually made a choice because of some property of an instance or by an inherent fact of a certain class being over/underrepresented"
online learning	predicting the right output given some input without affecting what we see in the future ; every observed input requires a prediction but also serves as a new example → we are predicting and learning at the same time
outliers	"Outliers are values in our data that take on unusual/unexpected values ; they can be classed into <span style=""font-weight:600"">unnatural </span>or <span style=""font-weight:600"">natural </span>; natural outliers exist because of a <span style=""font-weight:600"">genuine property of the data </span>(e.g. wealth distribution in capitalist countries 🗿) or because of some <span style=""font-weight:600"">genuine error of the data.</span>"
assumption of normality	Many machine learning algorithms assume data follows a normal distribution, this is not true in the case of natural outliers here we need to avoid the normality assumption since this would discard outliers
when to keep / remove outliers	keep if they are a natural part of the data distribution or also present in production (test set) ; remove if they are an error of the data and also not present in production
sample space	Set of all possible outcomes of an experiment, generally denoted by&nbsp;\(\Omega\), e.g. for a coinflip we get the following discrete space&nbsp;\(\Omega=\{H, T\}\)​
event space	Set of all possible subsets of the sample space, aka the power set of the discrete sample space. In the case of a continuous sample space its the set of all intervals.
discrete (event/sample) space	A sample/event space is discrete if its finitely countable, for example the coinflip sample space is discrete because there are clearly only 2 outcomes possible
continuous (event/sample) space	A sample/event space is continuous if its uncountably infinite e.g. \Omega=\R for a continuous random variable
perceptron	A perceptron is a machine learning model with a number of inputs, generally x1, x2, . . . , xn, and a single output. The output is a step function (function which outputs 1 or another value depending on some condition) of a linear combination of the inputs.
simple perceptron composition	"A simple composition of neurons does <span style=""font-weight:600"">not make them more powerful</span>. The function y generated by a composition of neurons is still a linear function of the inputs. To combine perceptron in a way more expressive than a single perceptron there <span style=""font-weight:600"">needs to be non-linearity introduced</span> in the <span style=""font-weight:600"">form of activation functions</span>"
"<span style=""font-weight:600"">hypothesis boosting</span>"	"Hypothesis boosting is a <span style=""font-weight:600"">bias mitigation technique </span>where we combine multiple models with a generally high bias to form an ensemble with a <span style=""font-weight:600"">lower bias </span>but at the cost of a <span style=""font-weight:600"">potentially higher variance</span>"
deep learning	Subset of machine learning that refers to multilayer perceptron models that generally employ different systems (mainly automatic differentiation) as a means of learning complex features from data
deep learning system / automatic differentiation	A deep learning system refers to a ML model which computes a forward pass ( outputs of the model ), using something like a computation graph and then a backward pass ( using backpropagation )
lazy execution/evaluation	"for lazy execution you : define computation graph → <span style=""font-weight:600"">don’t place data </span>in it → <span style=""font-weight:600"">compile computation graph </span>and start feeding data through it ; its fast, serializable and optimizable ; downsides are <span style=""font-weight:600"">difficult to debug</span>, model <span style=""font-weight:600"">must remain static during training</span>"
eager execution/evaluation	"for eager execution you : use programming statements to compute forward pass → computation graph built on the fly ; its easy to debug, problems occur during execution and its flexible; downsides are its <span style=""font-weight:600"">difficult to optimize, difficult to serialize</span>"
computation graph	"a graph representation of a computation (e.g. function) ; <span style=""font-weight:600"">circle nodes are inputs and outputs </span>; <span style=""font-weight:600"">diamond nodes are computations / functions</span>"
numeric approximation	the approximation of a solution to a problem especially in cases where no analytical ( exact ) method of finding a solution exists
activation function	Activation functions are used to introduce non-linearity into the output of a neuron. The most common activation functions are the sigmoid, and nowadays the rectified linear unit (ReLU).
ReLU activation function	The ReLU function is defined as the max of 0 and the input. If the input is 0 or negative, the output is 0, otherwise the output is the input.
sigmoid activation function	For logistic sigmoid the inputs can be any value and the output is constrained to [0, 1].
vanishing gradient	The vanishing gradient problem refers to the issue of weight initialization when using the sigmoid activation function ; too high weights sigmoid hits 1 , to low weights sigmoid hits 0 ; this is solved my ReLU
reducing vanishing gradient	We reduce vanishing gradients via ReLU ; ReLU activation function lets kills activation for nodes that produce a negative value and otherwise lets the activations through
tensor	different name for a multidimensional array (e.g. vector = 1D = 1-tensor, array = 2D = 2-tensor)
local derivatives	the partial derivatives of the different modules (parts) of a function which has been transformed into a composition
generator network	A model which learns a joint probability&nbsp;\(P(X, Y)\)&nbsp;for some instance Y and target Y​
mode collapse	mode collapse in generator networks refers to the case where the network generates only from a subset of the learn distribution so it ends up collapsing to producing a few or even a single averaged example repeatedly ; can also happen in GANs in the discriminator gets stuck on a local minima
GAN	"a generative adversarial network is a ml model which in which the generator portion generate an example of some instance (e.g. a face) this is then fed along with a real example (e.g. picture of some face) into a discriminator model, who’s job is it to distinguish the real from the fake input ; if the discriminator fails its update if it succeeds the generator model is updated ; a GAN mitigates mode collapse by<span style=""font-weight:600""> pushing the generator to produce more diverse ( realistic ) samples</span>"
KL-loss	KL-loss or KL-divergence is a measure of how far apart two distributions its computed by subtracting the joint entropy of the distributions we are comparing by the entropy of one of the distributiosn
cycle-consistency loss term	loss term used in a different type of GAN called CycleGAN
decision tree algorithm	"start with empty tree → extend step by step adding internal nodes ( greedy so <span style=""font-weight:600"">no backtracking</span>) → choose split that creates <span style=""font-weight:600"">least uniform distribution </span>over class labels ; <span style=""font-weight:600"">prone to overfitting</span> by itself"
decision tree nodes	features
decision tree leaves	targets
decision tree edges	types of features (e.g. node ⇒ genre , edge ⇒ romance / drama / comedy)
pruning decision trees	"a <span style=""font-weight:600"">overfitting mitigation technique </span>for <span style=""font-weight:600"">decision tree models </span>where - after training <span style=""font-weight:600"">using the validation data</span> - we work backwards from the leaves : for each leaf we see if DT is better with or without leaf → if better without we remove node → we keep repeating until performance increase stops"
validation set	the portion of the training data used to tune the model hyperparameters during the model tuning phase
confidence interval	A confidence interval in the context of a machine learning model is an interval on the distribution of sample accuracy, that is, the distribution of the probability of the proportion of correctly classified instances for different samples of the tests data
sample accuracy distribution	say we have a test set of 30 instances, we run 10 trails each trail we sample 10 instances from the test set and write down the number of true positives, the model gets from this test set, this should give us something like {1, 1, 3, 4, 7, 7, 7, 7, 7, 8, 9, 10} in the distribution 7 is clearly around the mean with a drop off on either side ; this indicates how likely a certain classification choice is to be by chance {1} or due to some inherent property of the model {7}
recommender system	A system which abstractly uses two sets of objects with relations such as a user and watched movies to come up with predictions
implicit feedback	Associations between users and items assumed from user behavior
Euclidian space	Just means like any numeric space like \R^2
Word2Vec	Technique in machine learning of obtaining a vector representation of words which after training could encode meaning, it learns a vector embedding of the words
Matrix factorization	A type of embedding model which learns an embedding matrix for a user and an item they like for example ; used in recommender systems
Embedding methods	These are methods of learning encoded representations that can convey meaning, generally this means encoding things as initially random vectors/arrays which we iteratively update
non-linear model	A non-linear model is one that can divide up the feature space in a non-linear fashion ; non-linearity is generally introduced through an activation function, this enables the non-linear separation&nbsp;
linear model	"A linear problem is one that requires a linearly separable feature space ; important <span style=""font-weight:600"">sometimes non-linear feature spaces → converted to linear feature space </span>through <span style=""font-weight:600"">derived features </span><b>&nbsp;</b>"
applications of derived features	We can use derived features to express a non-linear feature space as one which is linearly separable&nbsp;
frequentist learning	In frequentist learning we are given some data and our job is to guess the true model (out of a set of models) that generated some data. In other words, we want to pick the right θ so that the probability distribution fits the data best.
Bayesian learning	In Bayesian learning we are define the probability to a hypothesis given some evidence as a function of the probability we know (evidence) given the hypothesis times the probability of our hypothesis happening, and then divided by the “model evidence” which constrains us to the space of all possible hypothesis
Laplace smoothing	"Naive Bayes can run into issues with for <span style=""font-weight:600"">some features</span> a particular <span style=""font-weight:600"">value does not occur</span> thus giving a <span style=""font-weight:600"">probability of 0</span>. Its a problem because it <span style=""font-weight:600"">causes the entire naive Bayes probability to be 0</span>. We can solve this by using smoothing."
pseudo-observations	Pseudo-observations are a type of smoothing where for each possible value, we add one instance where all features have that value. In a large dataset the impact of this is minimal and there exist further techniques to minimize the impact even more
naive Bayes	Just Bayes but we take the product of a bunch of independent random variables conditioned on the same thing
L1	"Regularizer with the same idea as L2 just adding penalty for larger weights and <span style=""font-weight:600"">encouraging smaller weights ; </span>it is also known as a <span style=""font-weight:600"">sparsity-enforcing regularizer ; </span>sets weights to exactly 0"
L2	"Type of regularizer ( overfitting minimizer ) that adds a penalty loss for models with larger weights ( to simplify the model ) ; shrinks weights <span style=""font-weight:600"">but does not set them exactly to zero&nbsp;</span>"
Dropout	Type of regularizer that removes hidden and input nodes as a means of simplifying a model
Regularization	"An <span style=""font-weight:600"">overfitting mitigation technique </span>which pulls larger models towards simply ones without entirely removing complexity"
stride	"the length by which we move our kernel across some data, a <span style=""font-weight:600"">larger stride</span> means a<span style=""font-weight:600""> lower output resolution</span>"
padding	kernel parameter ; add extra units with 0 to either end of our kernel ; equalizes the number of inputs nd outputs
kernel size	defines the number of nodes our kernel like reduces from, so like if we have 3 input nodes connected to a single output node that’s a kernel size of 3
kernel output channels	kernel parameter ; defines the number of outputs our kernel maps to i think
convolutional layer	a mapping of our kernel to some set of output nodes
kernel parameters	the most important kernel parameters are : stride, padding, kernel size, kernel output channels
kernel output resolution	this is the resolution of our data after we applied the kernel, its effected by the stride, higher stride ⇒ lower output resolution and vv.
same padding	the technique of setting the number of units padded on both sides to&nbsp;\(\text{floor}(\text{kernel size}/ 2)\)&nbsp;to get the same number of inputs and outputs
expectation-maximization algorithm	the algorithm used to find the model (configuration) which gives the highest probability for a specific data
maximum likelihood	model criteria which maximizes the probability of seeing a specific data
univariate normal distribution	the normal distribution over a single random variable
multivariate normal distribution	the normal distribution across multiple random variables
gaussian mixture models	probabilistic models that assume all data points are generated by mixture of a finite number of Gaussian distributions with some unknown parameters
